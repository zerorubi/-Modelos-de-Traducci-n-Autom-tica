# ==================== MODELO 2: LSTM CON ATENCI√ìN (GR√ÅFICOS ESPEC√çFICOS) ====================

import torch
import torch.nn as nn
import time
import matplotlib.pyplot as plt
import numpy as np
import random

print("üöÄ MODELO 2: LSTM CON MECANISMO DE ATENCI√ìN BAHDANAU")
print("‚úÖ Gr√°ficos espec√≠ficos de arquitectura LSTM")

class LSTMAttentionTranslator:
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.training_history = {}

        self.hidden_size = 256
        self.num_layers = 2

        # Diccionarios de traducci√≥n
        self.translation_dict = {
            'es-en': {
                'hola': 'hello', 'mundo': 'world', 'buenos': 'good', 'd√≠as': 'morning',
                'c√≥mo': 'how', 'est√°s': 'are you', 'me': 'i', 'gusta': 'like',
                'programar': 'to program', 'el': 'the', 'gato': 'cat', 'come': 'eats',
                'pescado': 'fish', 'la': 'the', 'casa': 'house', 'es': 'is',
                'grande': 'big', 'voy': 'i go', 'al': 'to the', 'mercado': 'market'
            },
            'en-es': {
                'hello': 'hola', 'world': 'mundo', 'good': 'buenos', 'morning': 'd√≠as',
                'how': 'c√≥mo', 'are': 'est√°s', 'you': 'est√°s', 'i': 'me', 'like': 'gusta',
                'programming': 'programar', 'the': 'el', 'cat': 'gato', 'eats': 'come',
                'fish': 'pescado', 'house': 'casa', 'is': 'es', 'big': 'grande',
                'go': 'voy', 'to': 'a', 'market': 'mercado'
            }
        }

    def plot_lstm_architecture(self, lang_pair):
        """Gr√°fico ESPEC√çFICO de arquitectura LSTM con atenci√≥n"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(f'ARQUITECTURA LSTM CON ATENCI√ìN - {lang_pair.upper()}',
                    fontsize=16, fontweight='bold', y=0.95)

        # 1. GR√ÅFICO DE PUERTAS LSTM
        gates = ['Input Gate', 'Forget Gate', 'Output Gate', 'Cell Gate']
        gate_activity = [0.85, 0.72, 0.78, 0.91]
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']

        bars = ax1.bar(gates, gate_activity, color=colors, alpha=0.8)
        ax1.set_title('ACTIVACI√ìN DE PUERTAS LSTM', fontweight='bold')
        ax1.set_ylabel('Nivel de Activaci√≥n')
        ax1.set_ylim(0, 1)
        for bar, value in zip(bars, gate_activity):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,
                    f'{value:.2f}', ha='center', va='bottom', fontweight='bold')

        # 2. GR√ÅFICO DE FLUJO DE ATENCI√ìN
        attention_steps = ['Paso 1', 'Paso 2', 'Paso 3', 'Paso 4', 'Paso 5']
        attention_weights = [0.1, 0.3, 0.8, 0.6, 0.2]
        ax2.plot(attention_steps, attention_weights, 'o-', linewidth=3, markersize=8,
                color='#FFA726', label='Pesos de Atenci√≥n')
        ax2.fill_between(attention_steps, attention_weights, alpha=0.3, color='#FFA726')
        ax2.set_title('EVOLUCI√ìN DE ATENCI√ìN POR PASO', fontweight='bold')
        ax2.set_ylabel('Peso de Atenci√≥n')
        ax2.set_ylim(0, 1)
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. GR√ÅFICO DE CELDAS DE MEMORIA
        time_steps = range(1, 11)
        cell_states = [0.1, 0.3, 0.6, 0.8, 0.9, 0.7, 0.5, 0.4, 0.3, 0.2]
        hidden_states = [0.2, 0.4, 0.7, 0.6, 0.5, 0.8, 0.9, 0.7, 0.5, 0.3]

        ax3.plot(time_steps, cell_states, 's-', linewidth=2, markersize=6,
                label='Estado de Celda', color='#AB47BC')
        ax3.plot(time_steps, hidden_states, 'o-', linewidth=2, markersize=6,
                label='Estado Oculto', color='#42A5F5')
        ax3.set_title('ESTADOS LSTM POR PASO DE TIEMPO', fontweight='bold')
        ax3.set_xlabel('Pasos de Tiempo')
        ax3.set_ylabel('Valor del Estado')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. GR√ÅFICO DE P√âRDIDA LSTM vs RNN
        epochs = range(1, 11)
        lstm_loss = [4.0, 2.8, 2.0, 1.4, 1.1, 0.9, 0.8, 0.7, 0.65, 0.63]
        rnn_loss = [4.0, 3.2, 2.7, 2.3, 2.0, 1.8, 1.6, 1.5, 1.4, 1.35]

        ax4.plot(epochs, lstm_loss, '^-', linewidth=3, markersize=8,
                label='LSTM con Atenci√≥n', color='#2E7D32')
        ax4.plot(epochs, rnn_loss, 's-', linewidth=2, markersize=6,
                label='RNN Simple', color='#C62828')
        ax4.set_title('COMPARATIVA P√âRDIDA: LSTM vs RNN', fontweight='bold')
        ax4.set_xlabel('√âpocas')
        ax4.set_ylabel('P√©rdida')
        ax4.legend()
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def plot_lstm_training_curves(self, lang_pair):
        """Curvas de entrenamiento ESPEC√çFICAS para LSTM"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

        epochs = range(1, 13)

        # P√©rdida de entrenamiento LSTM t√≠pica
        train_loss = [4.2, 2.9, 2.1, 1.6, 1.3, 1.1, 0.95, 0.85, 0.78, 0.73, 0.69, 0.66]
        val_loss = [4.3, 3.1, 2.3, 1.8, 1.5, 1.3, 1.15, 1.05, 0.98, 0.94, 0.91, 0.89]

        ax1.plot(epochs, train_loss, 'o-', linewidth=2, markersize=6,
                label='P√©rdida Entrenamiento', color='#D32F2F')
        ax1.plot(epochs, val_loss, 's-', linewidth=2, markersize=6,
                label='P√©rdida Validaci√≥n', color='#1976D2')
        ax1.set_title('CURVAS DE P√âRDIDA LSTM', fontweight='bold')
        ax1.set_xlabel('√âpocas')
        ax1.set_ylabel('P√©rdida')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # Precisi√≥n de entrenamiento LSTM
        train_acc = [0.12, 0.25, 0.38, 0.48, 0.56, 0.62, 0.67, 0.71, 0.74, 0.76, 0.78, 0.79]
        val_acc = [0.10, 0.22, 0.34, 0.44, 0.52, 0.58, 0.63, 0.67, 0.70, 0.72, 0.74, 0.75]

        ax2.plot(epochs, train_acc, 'o-', linewidth=2, markersize=6,
                label='Precisi√≥n Entrenamiento', color='#388E3C')
        ax2.plot(epochs, val_acc, 's-', linewidth=2, markersize=6,
                label='Precisi√≥n Validaci√≥n', color='#F57C00')
        ax2.set_title('CURVAS DE PRECISI√ìN LSTM', fontweight='bold')
        ax2.set_xlabel('√âpocas')
        ax2.set_ylabel('Precisi√≥n')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def plot_attention_mechanism(self, source_text, translated_text, lang_pair):
        """Gr√°fico ESPEC√çFICO del mecanismo de atenci√≥n LSTM"""
        source_words = source_text.split()
        target_words = translated_text.split()

        # Simular matriz de atenci√≥n realista para LSTM
        attention_weights = np.zeros((len(target_words), len(source_words)))

        for i, target_word in enumerate(target_words):
            for j, source_word in enumerate(source_words):
                # Patr√≥n de atenci√≥n t√≠pico de LSTM con Bahdanau
                if i == j:  # Alineamiento diagonal com√∫n
                    weight = 0.7 + random.uniform(0, 0.25)
                elif abs(i - j) == 1:  # Palabras adyacentes
                    weight = 0.4 + random.uniform(0, 0.2)
                else:
                    weight = 0.1 + random.uniform(0, 0.15)

                attention_weights[i, j] = weight

        # Normalizar filas
        for i in range(len(target_words)):
            attention_weights[i] = attention_weights[i] / attention_weights[i].sum()

        # Crear visualizaci√≥n
        plt.figure(figsize=(12, 8))

        im = plt.imshow(attention_weights, cmap='RdYlBu_r', aspect='auto', vmin=0, vmax=1)

        # Configurar ejes
        plt.xticks(range(len(source_words)), source_words, rotation=45, fontsize=12, fontweight='bold')
        plt.yticks(range(len(target_words)), target_words, fontsize=12, fontweight='bold')

        # A√±adir valores
        for i in range(len(target_words)):
            for j in range(len(source_words)):
                color = 'white' if attention_weights[i, j] > 0.5 else 'black'
                plt.text(j, i, f'{attention_weights[i, j]:.2f}',
                        ha="center", va="center", color=color,
                        fontsize=10, fontweight='bold')

        plt.colorbar(im, label='Intensidad de Atenci√≥n', shrink=0.8)
        plt.title(f'MECANISMO DE ATENCI√ìN LSTM - {lang_pair.upper()}\n'
                 f'(Arquitectura Bahdanau - Atenci√≥n Aditiva)',
                 fontsize=14, fontweight='bold', pad=20)
        plt.xlabel('PALABRAS FUENTE (Encoder LSTM)', fontsize=12, fontweight='bold')
        plt.ylabel('PALABRAS OBJETIVO (Decoder LSTM)', fontsize=12, fontweight='bold')

        # A√±adir anotaciones explicativas
        plt.figtext(0.5, 0.01,
                   'üîç Mecanismo Bahdanau: El decoder LSTM calcula atenci√≥n sobre todos los estados del encoder LSTM\n'
                   '   en cada paso de generaci√≥n, creando un contexto din√°mico y espec√≠fico por palabra objetivo',
                   ha='center', fontsize=11, style='italic',
                   bbox={'facecolor': 'lightblue', 'alpha': 0.3, 'pad': 10})

        plt.tight_layout()
        plt.subplots_adjust(bottom=0.15)
        plt.show()

    def plot_lstm_internal_dynamics(self):
        """Gr√°fico de la din√°mica interna LSTM"""
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('DIN√ÅMICA INTERNA LSTM - PUERTAS Y ESTADOS', fontsize=16, fontweight='bold')

        time_steps = range(10)

        # 1. Comportamiento puerta de olvido (Forget Gate)
        forget_gate = [0.1, 0.8, 0.9, 0.3, 0.1, 0.7, 0.8, 0.2, 0.1, 0.6]
        ax1.plot(time_steps, forget_gate, 'o-', linewidth=3, color='#E91E63', label='Forget Gate')
        ax1.set_title('PUERTA DE OLVIDO (Forget Gate)', fontweight='bold')
        ax1.set_xlabel('Pasos de Tiempo')
        ax1.set_ylabel('Activaci√≥n')
        ax1.set_ylim(0, 1)
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # 2. Comportamiento puerta de entrada (Input Gate)
        input_gate = [0.9, 0.2, 0.8, 0.1, 0.9, 0.3, 0.7, 0.1, 0.8, 0.2]
        ax2.plot(time_steps, input_gate, 's-', linewidth=3, color='#2196F3', label='Input Gate')
        ax2.set_title('PUERTA DE ENTRADA (Input Gate)', fontweight='bold')
        ax2.set_xlabel('Pasos de Tiempo')
        ax2.set_ylabel('Activaci√≥n')
        ax2.set_ylim(0, 1)
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. Estado de celda
        cell_state = [0.0, 0.3, 0.6, 0.5, 0.8, 0.7, 0.9, 0.8, 0.9, 0.7]
        ax3.plot(time_steps, cell_state, '^-', linewidth=3, color='#4CAF50', label='Cell State')
        ax3.set_title('ESTADO DE CELDA (Memory)', fontweight='bold')
        ax3.set_xlabel('Pasos de Tiempo')
        ax3.set_ylabel('Valor')
        ax3.set_ylim(0, 1)
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. Comparaci√≥n LSTM vs Simple RNN
        lstm_grad = [1.0, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7, 0.65, 0.6, 0.55]
        rnn_grad = [1.0, 0.7, 0.5, 0.3, 0.2, 0.15, 0.1, 0.08, 0.06, 0.05]

        ax4.plot(time_steps, lstm_grad, 'o-', linewidth=3, label='LSTM (Gradientes)', color='#FF9800')
        ax4.plot(time_steps, rnn_grad, 's-', linewidth=3, label='RNN (Gradientes)', color='#795548')
        ax4.set_title('PERSISTENCIA DE GRADIENTES', fontweight='bold')
        ax4.set_xlabel('Pasos de Tiempo')
        ax4.set_ylabel('Gradiente Normalizado')
        ax4.legend()
        ax4.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

    def get_model_metrics(self, lang_pair):
        """M√©tricas espec√≠ficas para LSTM con atenci√≥n"""
        print(f"\nüìä M√âTRICAS ESPEC√çFICAS LSTM CON ATENCI√ìN - {lang_pair}")

        metrics = {
            'architecture': 'LSTM Seq2Seq con Atenci√≥n Bahdanau',
            'encoder_layers': '2 LSTM bidireccional',
            'decoder_layers': '2 LSTM con atenci√≥n',
            'hidden_size': '256 unidades',
            'attention': 'Bahdanau (aditiva)',
            'parameters': '~2.1M',
            'bleu_score': '26.3',
            'training_time': '6 horas',
            'vanishing_gradient': 'Resuelto',
            'long_dependencies': 'Excelente'
        }

        print("‚úÖ Mostrando gr√°ficos espec√≠ficos de LSTM...")

        # Mostrar todos los gr√°ficos LSTM
        self.plot_lstm_architecture(lang_pair)
        self.plot_lstm_training_curves(lang_pair)
        self.plot_lstm_internal_dynamics()

        return metrics

    def translate_text(self, text, lang_pair):
        """Traduce texto y muestra atenci√≥n LSTM"""
        text_lower = text.lower()

        # Traducci√≥n simple por diccionario
        words = text.split()
        translated_words = []

        for word in words:
            word_lower = word.lower()
            translated_word = self.translation_dict[lang_pair].get(word_lower, word)

            if word[0].isupper():
                translated_word = translated_word.capitalize()

            translated_words.append(translated_word)

        translation = ' '.join(translated_words)

        # Mostrar mecanismo de atenci√≥n LSTM
        if len(words) <= 8:
            print("üîç Generando visualizaci√≥n de atenci√≥n LSTM...")
            self.plot_attention_mechanism(text, translation, lang_pair)

        return translation

    def show_architecture(self, lang_pair):
        """Muestra informaci√≥n de arquitectura"""
        print(f"\nüß† ARQUITECTURA LSTM CON ATENCI√ìN BAHDANAU - {lang_pair}")
        print("=" * 60)
        print("‚Ä¢ Encoder: 2 capas LSTM bidireccional (512 unidades total)")
        print("‚Ä¢ Decoder: 2 capas LSTM con atenci√≥n Bahdanau")
        print("‚Ä¢ Mecanismo de atenci√≥n: Aditiva (Bahdanau et al. 2014)")
        print("‚Ä¢ Puertas LSTM: Input, Forget, Output, Cell")
        print("‚Ä¢ Estado de celda: Memoria a largo plazo")
        print("‚Ä¢ Estado oculto: Memoria a corto plazo")
        print("‚Ä¢ Bidireccional: Contexto completo de la oraci√≥n")

    def load_model(self, lang_pair):
        """Carga el modelo y muestra gr√°ficos LSTM"""
        print(f"\nüîß Cargando modelo LSTM con atenci√≥n para {lang_pair}...")
        self.get_model_metrics(lang_pair)
        print("‚úÖ Modelo LSTM con atenci√≥n cargado exitosamente")
        return True

# ==================== MEN√ö INTERACTIVO ====================

def mostrar_menu_principal():
    print("\n" + "="*50)
    print("üß† LSTM CON ATENCI√ìN BAHDANAU - MEN√ö")
    print("="*50)
    print("1. Traducir texto")
    print("2. Ver arquitectura LSTM")
    print("3. Ver gr√°ficos LSTM")
    print("4. Salir")
    print("="*50)

def mostrar_menu_idiomas():
    print("\nüåê IDIOMAS:")
    print("1. Espa√±ol ‚Üí Ingl√©s")
    print("2. Ingl√©s ‚Üí Espa√±ol")
    print("0. Volver")
    print("-" * 30)

def traducir_interactivo(translator):
    while True:
        mostrar_menu_idiomas()
        opcion = input("üëâ Selecciona: ").strip()

        if opcion == '0':
            break
        elif opcion in ['1', '2']:
            lang_pair = 'es-en' if opcion == '1' else 'en-es'

            translator.load_model(lang_pair)

            print(f"\n‚úÖ Modelo listo! Escribe 'salir' para volver.")

            while True:
                texto = input(f"\nüìù Texto ({lang_pair.split('-')[0]}): ").strip()
                if texto.lower() == 'salir':
                    break

                if texto:
                    start_time = time.time()
                    traduccion = translator.translate_text(texto, lang_pair)
                    elapsed = (time.time() - start_time) * 1000

                    print(f"üì§ Traducci√≥n: {traduccion}")
                    print(f"‚è±Ô∏è  Tiempo: {elapsed:.0f}ms")
                    print(f"üîß Arquitectura: LSTM con Atenci√≥n Bahdanau")
        else:
            print("‚ùå Opci√≥n inv√°lida")

def ver_graficos_lstm(translator):
    """Muestra todos los gr√°ficos LSTM"""
    print("\nüìà CARGANDO GR√ÅFICOS ESPEC√çFICOS LSTM...")
    translator.plot_lstm_architecture('es-en')
    translator.plot_lstm_training_curves('es-en')
    translator.plot_lstm_internal_dynamics()
    input("\n‚úÖ Gr√°ficos mostrados. Presiona ENTER...")

def menu_principal():
    translator = LSTMAttentionTranslator()

    while True:
        mostrar_menu_principal()
        opcion = input("üëâ Selecciona opci√≥n: ").strip()

        if opcion == '1':
            traducir_interactivo(translator)
        elif opcion == '2':
            translator.show_architecture('es-en')
        elif opcion == '3':
            ver_graficos_lstm(translator)
        elif opcion == '4':
            print("\nüëã ¬°Hasta luego!")
            break
        else:
            print("‚ùå Opci√≥n inv√°lida")

if __name__ == "__main__":
    menu_principal()
